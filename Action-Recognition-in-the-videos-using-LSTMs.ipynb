{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action recognition in video using LSTMs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements:\n",
    "\n",
    "First we need to download the UCF101 dataset and extract it. When done, change the `BASE_PATH` variable to point to the dataset folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Dataset through Kaggle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ~/.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ./kaggle.json ~/.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle.json\r\n"
     ]
    }
   ],
   "source": [
    "!cd ~/.kaggle/ && ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!kaggle datasets list -s ucf101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/ec2-user/.kaggle/kaggle.json'\n",
      "Downloading ucf101.zip to /home/ec2-user/SageMaker\n",
      "100%|███████████████████████████████████████| 6.49G/6.49G [00:22<00:00, 263MB/s]\n",
      "100%|███████████████████████████████████████| 6.49G/6.49G [00:22<00:00, 314MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d pevogam/ucf101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!unzip ucf101.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages in the current environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T16:00:52.595561Z",
     "start_time": "2019-05-26T16:00:50.075919Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install opencv-python \n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install tqdm\n",
    "!{sys.executable} -m pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib \n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow                         2.7.1\n",
      "tensorflow-estimator               2.7.0\n",
      "tensorflow-io-gcs-filesystem       0.24.0\n",
      "tensorflow-serving-api             2.7.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/tensorflow2_p38/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 list | grep tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T16:00:53.810865Z",
     "start_time": "2019-05-26T16:00:52.598200Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T16:00:53.813936Z",
     "start_time": "2019-05-26T16:00:53.811881Z"
    }
   },
   "outputs": [],
   "source": [
    "BASE_PATH = 'UCF101/UCF-101'\n",
    "VIDEOS_PATH = os.path.join(BASE_PATH, '**','*.avi')\n",
    "SEQUENCE_LENGTH = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UCF101/UCF-101'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UCF101/UCF-101/**/*.avi'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VIDEOS_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Extract features from videos and cache them in files:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate feature vectors, we will use a pretrained inception network trained on the ImageNet dataset to categorize images in different categories.\n",
    "We will remove the last layer (the fully connected layer) and only keep the feature vector that is generated after a max-pooling operation.\n",
    "Another option would be to keep the output of the layer just before average-pooling, that is, the higher-dimensional feature maps. However, in our example, we will not need spatial information—whether the action takes place in the middle of the frame or in the corner, the predictions will be the same. Therefore, we will use the output of the two-dimensional max-pooling layer. This will make the training faster, since the input of the LSTM will be 64 times smaller (64 = 8 × 8 = the size of a feature map for an input image of size 299 × 299)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T00:57:45.101242Z",
     "start_time": "2019-04-08T00:57:45.096277Z"
    }
   },
   "source": [
    "### Sample 'SEQUENCE_LENGTH' frames from each video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T16:00:54.250300Z",
     "start_time": "2019-05-26T16:00:53.815151Z"
    }
   },
   "outputs": [],
   "source": [
    "def frame_generator():\n",
    "    video_paths = tf.io.gfile.glob(VIDEOS_PATH)\n",
    "    np.random.shuffle(video_paths)\n",
    "    for video_path in video_paths:\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        sample_every_frame = max(1, num_frames // SEQUENCE_LENGTH)\n",
    "        current_frame = 0\n",
    "\n",
    "        label = os.path.basename(os.path.dirname(video_path))\n",
    "\n",
    "        max_images = SEQUENCE_LENGTH\n",
    "        while True:\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                break\n",
    "\n",
    "            if current_frame % sample_every_frame == 0:\n",
    "                # OPENCV reads in BGR, tensorflow expects RGB so we invert the order\n",
    "                frame = frame[:, :, ::-1]\n",
    "                img = tf.image.resize(frame, (299, 299))\n",
    "                img = tf.keras.applications.inception_v3.preprocess_input(\n",
    "                    img)\n",
    "                max_images -= 1\n",
    "                yield img, video_path\n",
    "\n",
    "            if max_images == 0:\n",
    "                break\n",
    "            current_frame += 1\n",
    "\n",
    "# `from_generator` might throw a warning, expected to disappear in upcoming versions:\n",
    "# https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#for_example_2\n",
    "dataset = tf.data.Dataset.from_generator(frame_generator,\n",
    "             output_types=(tf.float32, tf.string),\n",
    "             output_shapes=((299, 299, 3), ()))\n",
    "\n",
    "dataset = dataset.batch(16).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None, 299, 299, 3), (None,)), types: (tf.float32, tf.string)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow allows us to access a pretrained model with a single line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T16:00:58.352463Z",
     "start_time": "2019-05-26T16:00:54.251438Z"
    }
   },
   "outputs": [],
   "source": [
    "inception_v3 = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "\n",
    "x = inception_v3.output\n",
    "\n",
    "# We add Average Pooling to transform the feature map from \n",
    "# 8 * 8 * 2048 to 1 x 2048, as we don't need spatial information\n",
    "\n",
    "pooling_output = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "feature_extraction_model = tf.keras.Model(inception_v3.input, pooling_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features and store them in .npy files:\n",
    "\n",
    "Extraction takes about ~51minutes on an AWS Sagemaker using ml.g5.8xlarge instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T16:02:20.594553Z",
     "start_time": "2019-05-26T16:01:03.420149Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33295it [50:21, 11.02it/s]\n"
     ]
    }
   ],
   "source": [
    "current_path = None\n",
    "all_features = []\n",
    "\n",
    "for img, batch_paths in tqdm.tqdm(dataset):\n",
    "    batch_features = feature_extraction_model(img)\n",
    "    batch_features = tf.reshape(batch_features, \n",
    "                              (batch_features.shape[0], -1))\n",
    "    \n",
    "    for features, path in zip(batch_features.numpy(), batch_paths.numpy()):\n",
    "        if path != current_path and current_path is not None:\n",
    "            output_path = current_path.decode().replace('.avi', '.npy')\n",
    "            np.save(output_path, all_features)\n",
    "            all_features = []\n",
    "            \n",
    "        current_path = path\n",
    "        all_features.append(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T01:00:03.256769Z",
     "start_time": "2019-04-08T01:00:03.251932Z"
    }
   },
   "source": [
    "## Step 2: Train the LSTM on video features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the video features are generated, we can use them to train an LSTM. We define a model and an input pipeline, and launch the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T16:02:24.834899Z",
     "start_time": "2019-05-26T16:02:24.826062Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelBinarizer()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABELS = ['UnevenBars','ApplyLipstick','TableTennisShot','Fencing','Mixing','SumoWrestling','HulaHoop','PommelHorse','HorseRiding','SkyDiving','BenchPress','GolfSwing','HeadMassage','FrontCrawl','Haircut','HandstandWalking','Skiing','PlayingDaf','PlayingSitar','FrisbeeCatch','CliffDiving','BoxingSpeedBag','Kayaking','Rafting','WritingOnBoard','VolleyballSpiking','Archery','MoppingFloor','JumpRope','Lunges','BasketballDunk','Surfing','SkateBoarding','FloorGymnastics','Billiards','CuttingInKitchen','BlowingCandles','PlayingCello','JugglingBalls','Drumming','ThrowDiscus','BaseballPitch','SoccerPenalty','Hammering','BodyWeightSquats','SoccerJuggling','CricketShot','BandMarching','PlayingPiano','BreastStroke','ApplyEyeMakeup','HighJump','IceDancing','HandstandPushups','RockClimbingIndoor','HammerThrow','WallPushups','RopeClimbing','Basketball','Shotput','Nunchucks','WalkingWithDog','PlayingFlute','PlayingDhol','PullUps','CricketBowling','BabyCrawling','Diving','TaiChi','YoYo','BlowDryHair','PushUps','ShavingBeard','Knitting','HorseRace','TrampolineJumping','Typing','Bowling','CleanAndJerk','MilitaryParade','FieldHockeyPenalty','PlayingViolin','Skijet','PizzaTossing','LongJump','PlayingTabla','PlayingGuitar','BrushingTeeth','PoleVault','Punch','ParallelBars','Biking','BalanceBeam','Swing','JavelinThrow','Rowing','StillRings','SalsaSpin','TennisSwing','JumpingJack','BoxingPunchingBag'] \n",
    "encoder = LabelBinarizer()\n",
    "\n",
    "encoder.fit(LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply a dropout. The dropout parameter of the LSTM controls how much dropout is applied to the input weight matrix. The recurrent_dropout parameter controls how much dropout is applied to the previous state. Similar to a mask, recurrent_dropout randomly ignores part of the previous state activations in order to avoid overfitting.\n",
    "The very first layer of our model is a Masking layer. As we padded our image sequences with empty frames in order to batch them, our LSTM cell would needlessly iterate over those added frames. Adding the Masking layer ensures the LSTM layer stops at the actual end of the sequence, before it encounters a zero matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T17:54:21.102700Z",
     "start_time": "2019-05-26T17:54:21.094574Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Masking(mask_value=0.),\n",
    "    tf.keras.layers.LSTM(512, dropout=0.5, recurrent_dropout=0.5),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(len(LABELS), activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T17:54:21.368352Z",
     "start_time": "2019-05-26T17:54:21.362664Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy', 'top_k_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Model.summary of <keras.engine.sequential.Sequential object at 0x7f74ec09adf0>>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load the .npy files that are produced when generating frame features using a generator. The code ensures that all the input sequences have the same length, padding them with zeros if necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T17:54:21.615229Z",
     "start_time": "2019-05-26T17:54:21.594034Z"
    }
   },
   "outputs": [],
   "source": [
    "test_file = os.path.join('UCF101TrainTestSplits-RecognitionTask/ucfTrainTestlist', 'testlist01.txt')\n",
    "train_file = os.path.join('UCF101TrainTestSplits-RecognitionTask/ucfTrainTestlist', 'trainlist01.txt')\n",
    "\n",
    "with open('UCF101TrainTestSplits-RecognitionTask/ucfTrainTestlist/testlist01.txt') as f:\n",
    "    test_list = [row.strip() for row in list(f)]\n",
    "\n",
    "with open('UCF101TrainTestSplits-RecognitionTask/ucfTrainTestlist/trainlist01.txt') as f:\n",
    "    train_list = [row.strip() for row in list(f)]\n",
    "    train_list = [row.split(' ')[0] for row in train_list]\n",
    "\n",
    "\n",
    "def make_generator(file_list):\n",
    "    def generator():\n",
    "        np.random.shuffle(file_list)\n",
    "        for path in file_list:\n",
    "            full_path = os.path.join(BASE_PATH, path).replace('.avi', '.npy')\n",
    "\n",
    "            label = os.path.basename(os.path.dirname(path))\n",
    "            features = np.load(full_path)\n",
    "\n",
    "            padded_sequence = np.zeros((SEQUENCE_LENGTH, 2048))\n",
    "            padded_sequence[0:len(features)] = np.array(features)\n",
    "\n",
    "            transformed_label = encoder.transform([label])\n",
    "            yield padded_sequence, transformed_label[0]\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UCF101TrainTestSplits-RecognitionTask/ucfTrainTestlist/trainlist01.txt'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UCF101TrainTestSplits-RecognitionTask/ucfTrainTestlist/testlist01.txt'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_generator(make_generator(train_list),\n",
    "                 output_types=(tf.float32, tf.int16),\n",
    "                 output_shapes=((SEQUENCE_LENGTH, 2048), (len(LABELS))))\n",
    "\n",
    "train_dataset = train_dataset.batch(16).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_generator(make_generator(test_list),\n",
    "                 output_types=(tf.float32, tf.int16),\n",
    "                 output_shapes=((SEQUENCE_LENGTH, 2048), (len(LABELS))))\n",
    "valid_dataset = valid_dataset.batch(16).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='/tmp', update_freq=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/17\n",
      "597/597 [==============================] - 91s 153ms/step - loss: 2.1660 - accuracy: 0.4207 - top_k_categorical_accuracy: 0.7234 - val_loss: 1.9832 - val_accuracy: 0.4681 - val_top_k_categorical_accuracy: 0.7761\n",
      "Epoch 2/17\n",
      "597/597 [==============================] - 90s 152ms/step - loss: 1.4565 - accuracy: 0.5873 - top_k_categorical_accuracy: 0.8643 - val_loss: 1.3037 - val_accuracy: 0.6376 - val_top_k_categorical_accuracy: 0.8766\n",
      "Epoch 3/17\n",
      "597/597 [==============================] - 90s 151ms/step - loss: 1.1055 - accuracy: 0.6717 - top_k_categorical_accuracy: 0.9128 - val_loss: 1.6159 - val_accuracy: 0.5913 - val_top_k_categorical_accuracy: 0.8512\n",
      "Epoch 4/17\n",
      "597/597 [==============================] - 90s 152ms/step - loss: 0.8938 - accuracy: 0.7322 - top_k_categorical_accuracy: 0.9388 - val_loss: 1.3537 - val_accuracy: 0.6431 - val_top_k_categorical_accuracy: 0.8834\n",
      "Epoch 5/17\n",
      "597/597 [==============================] - 90s 151ms/step - loss: 0.7593 - accuracy: 0.7726 - top_k_categorical_accuracy: 0.9518 - val_loss: 1.6356 - val_accuracy: 0.6001 - val_top_k_categorical_accuracy: 0.8327\n",
      "Epoch 6/17\n",
      "597/597 [==============================] - 91s 152ms/step - loss: 0.6741 - accuracy: 0.7956 - top_k_categorical_accuracy: 0.9630 - val_loss: 1.5168 - val_accuracy: 0.6487 - val_top_k_categorical_accuracy: 0.8686\n",
      "Epoch 7/17\n",
      "597/597 [==============================] - 91s 152ms/step - loss: 0.5720 - accuracy: 0.8274 - top_k_categorical_accuracy: 0.9731 - val_loss: 1.6563 - val_accuracy: 0.6577 - val_top_k_categorical_accuracy: 0.8736\n",
      "Epoch 8/17\n",
      "597/597 [==============================] - 90s 151ms/step - loss: 0.5065 - accuracy: 0.8467 - top_k_categorical_accuracy: 0.9801 - val_loss: 1.6443 - val_accuracy: 0.6524 - val_top_k_categorical_accuracy: 0.8763\n",
      "Epoch 9/17\n",
      "597/597 [==============================] - 90s 152ms/step - loss: 0.4674 - accuracy: 0.8599 - top_k_categorical_accuracy: 0.9818 - val_loss: 1.3213 - val_accuracy: 0.6767 - val_top_k_categorical_accuracy: 0.8982\n",
      "Epoch 10/17\n",
      "597/597 [==============================] - 90s 151ms/step - loss: 0.3991 - accuracy: 0.8825 - top_k_categorical_accuracy: 0.9875 - val_loss: 1.4467 - val_accuracy: 0.6693 - val_top_k_categorical_accuracy: 0.8871\n",
      "Epoch 11/17\n",
      "597/597 [==============================] - 90s 152ms/step - loss: 0.3710 - accuracy: 0.8901 - top_k_categorical_accuracy: 0.9875 - val_loss: 1.3070 - val_accuracy: 0.7111 - val_top_k_categorical_accuracy: 0.9035\n",
      "Epoch 12/17\n",
      "597/597 [==============================] - 90s 151ms/step - loss: 0.3530 - accuracy: 0.8954 - top_k_categorical_accuracy: 0.9897 - val_loss: 1.7491 - val_accuracy: 0.6603 - val_top_k_categorical_accuracy: 0.8887\n",
      "Epoch 13/17\n",
      "597/597 [==============================] - 91s 152ms/step - loss: 0.3285 - accuracy: 0.9012 - top_k_categorical_accuracy: 0.9921 - val_loss: 1.5829 - val_accuracy: 0.6743 - val_top_k_categorical_accuracy: 0.9001\n",
      "Epoch 14/17\n",
      "597/597 [==============================] - 91s 152ms/step - loss: 0.3055 - accuracy: 0.9099 - top_k_categorical_accuracy: 0.9912 - val_loss: 1.7458 - val_accuracy: 0.6868 - val_top_k_categorical_accuracy: 0.8916\n",
      "Epoch 15/17\n",
      "597/597 [==============================] - 91s 152ms/step - loss: 0.2892 - accuracy: 0.9142 - top_k_categorical_accuracy: 0.9940 - val_loss: 1.3935 - val_accuracy: 0.6981 - val_top_k_categorical_accuracy: 0.8996\n",
      "Epoch 16/17\n",
      "597/597 [==============================] - 91s 152ms/step - loss: 0.2995 - accuracy: 0.9132 - top_k_categorical_accuracy: 0.9939 - val_loss: 1.6280 - val_accuracy: 0.6913 - val_top_k_categorical_accuracy: 0.9006\n",
      "Epoch 17/17\n",
      "597/597 [==============================] - 91s 152ms/step - loss: 0.2808 - accuracy: 0.9182 - top_k_categorical_accuracy: 0.9940 - val_loss: 1.5186 - val_accuracy: 0.6860 - val_top_k_categorical_accuracy: 0.9033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f74ec066d30>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, \n",
    "          epochs=17, \n",
    "          callbacks=[tensorboard_callback], \n",
    "          validation_data=valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "notify_time": "5",
  "vscode": {
   "interpreter": {
    "hash": "264629b8f0b9dfb35a7415cbbfec03d82aafb372d3e2d7dba3a7a6618a988f3c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
